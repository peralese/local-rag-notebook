# Local RAG ‚Äî Fastembed + Chroma + FastAPI

A tiny, CPU‚Äëfriendly local document query service. It uses **fastembed** (`BAAI/bge-small-en-v1.5`) for embeddings, **ChromaDB** for the vector store, and **FastAPI** for an HTTP interface (with Swagger UI). Dockerized for a dead‚Äësimple start.

---

## Features
- ‚úÖ CPU‚Äëonly embeddings via **fastembed** (ThinkPad‚Äëfriendly)
- ‚úÖ Persistent **Chroma** index under `./data/chroma`
- ‚úÖ Simple API: **/reset**, **/ingest_all**, **/ingest_file**, **/count**, **/query**
- ‚úÖ `docker compose up -d --build web` and you're running
- ‚ôªÔ∏è Embedding backend can be flipped to **Ollama** later if needed

---

## Project Layout
```
repo-root/
‚îú‚îÄ docker-compose.yml
‚îú‚îÄ data/                 # mounted to /data in the container
‚îÇ  ‚îî‚îÄ uploads/           # put your files here to ingest
‚îú‚îÄ web/
‚îÇ  ‚îú‚îÄ app.py             # FastAPI service
‚îÇ  ‚îú‚îÄ embeddings.py      # fastembed (default) / optional ollama backend
‚îÇ  ‚îú‚îÄ ingest.py          # minimal text-file ingester
‚îÇ  ‚îú‚îÄ vectorstore.py     # Chroma wrapper
‚îÇ  ‚îú‚îÄ requirements.txt
‚îÇ  ‚îî‚îÄ Dockerfile
‚îî‚îÄ README.md
```
> Tip: add a placeholder `data/uploads/.gitkeep` so the folder exists in fresh clones (the `.gitignore` is set up for this).

---

## Requirements
- Docker & Docker Compose v2
- Ports: `8000` available locally

---

## Quick Start
1. **Put a test file** on the host at `data/uploads/small.txt`:
   ```text
   There is a secret code hidden here: ALPHA-42.
   I also like apples.
   ```

2. **Build & run**:
   ```bash
   docker compose up -d --build web
   ```

3. **Open the API docs**: http://localhost:8000/docs

4. **Reset** ‚Üí **Ingest** ‚Üí **Query** (via Swagger UI):
   - `POST /reset` ‚Üí Execute
   - `POST /ingest_all` ‚Üí Execute
   - `GET /count` ‚Üí should be `> 0`
   - `GET /query` with `q=secret code` ‚Üí you should see the chunk with `ALPHA-42`

### CLI equivalents (PowerShell/CMD-safe)
```bash
# reset
curl -X POST http://localhost:8000/reset

# ingest everything under /data/uploads
curl -X POST http://localhost:8000/ingest_all

# count
curl http://localhost:8000/count

# query
curl "http://localhost:8000/query?q=secret%20code&k=5"
```

---

## Configuration
Environment variables (set in `docker-compose.yml`):
- `EMBED_BACKEND` ‚Äî `fastembed` (default) or `ollama`
- `EMBED_MODEL` ‚Äî default `BAAI/bge-small-en-v1.5`
- `VECTOR_DIR` ‚Äî default `/data/chroma`

> **Note on responses:** `embeddings` may show as `null` unless explicitly requested; Chroma still uses them internally for search.

### Switch to Ollama later (optional)
If you want to try Ollama embeddings again:
1. Run an Ollama container or service reachable from the `web` container.
2. Set env vars on `web`:
   ```yaml
   EMBED_BACKEND=ollama
   EMBED_MODEL=nomic-embed-text
   OLLAMA_HOST=http://ollama:11434
   ```
3. Rebuild `web`:
   ```bash
   docker compose up -d --no-deps --build web
   ```

---

## Supported Files
The reference `ingest.py` reads **plain text** formats (txt/md/py/etc.). Add handlers for PDFs/Docs when needed (see Roadmap).

---

## Troubleshooting
- **Windows PowerShell + heredocs**: not used here; all interactions go through HTTP or simple `curl`.
- **UTF‚Äë8 BOM (`√Ø¬ª¬ø`)**: save helper scripts as **UTF‚Äë8 without BOM** or ANSI to avoid BOM issues.
- **`ModuleNotFoundError: vectorstore`**: always interact through the API, or run Python with the working dir that contains `vectorstore.py` (the service already does this).

---

## Roadmap (Recommended)
### Phase 1 ‚Äî Core RAG polish
- [ ] Robust chunking (e.g., Markdown-aware, paragraph/sentence split, overlap)
- [ ] Metadata capture (filename, path, timestamp, source URL)
- [ ] Duplicate detection & upsert (content hash)
- [ ] Better query response: include `ids`, `distances`, and top‚Äëk docs in output
- [ ] Simple `/answer` endpoint (extract short answer from top hit ‚Äî no LLM)
- [ ] Basic unit tests for `VectorStore` and `ingest`

### Phase 2 ‚Äî More formats & quality
- [ ] PDF via `pypdf` or `pdfminer.six` (text‚Äëonly to start)
- [ ] DOCX via `python-docx`
- [ ] HTML parsing (readability + boilerplate removal)
- [ ] Optional re‚Äëranking step (e.g., cross‚Äëencoder) for better precision

### Phase 3 ‚Äî UX & ops
- [ ] Minimal web UI (drag‚Äëdrop ingest, search box, results pane)
- [ ] Auth (API key or local login)
- [ ] Structured logging & basic metrics
- [ ] Backup/restore for Chroma index
- [ ] Background re‚Äëingest/refresh job

### Phase 4 ‚Äî LLM answer synthesis (optional)
- [ ] Answer composition from retrieved context
- [ ] Citations (spans back to source chunks)
- [ ] Safety/guardrails (max tokens, cost controls if using external LLMs)

---

## License
MIT (suggested). Update as appropriate.

---

## Acknowledgements
- BAAI/bge-small-en-v1.5 ‚Äî https://huggingface.co/BAAI/bge-small-en-v1.5
- ChromaDB ‚Äî https://www.trychroma.com/
- FastAPI ‚Äî https://fastapi.tiangolo.com/

## üë®‚Äçüíª Author

Erick Perales  
Cloud Migration IT Architect, Cloud Migration Specialist
[https://github.com/peralese](https://github.com/peralese)